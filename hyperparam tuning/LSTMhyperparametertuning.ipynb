{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c9d8a07c10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchinfo import summary\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import pennylane as qml\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('../assets/processed dataset/train dataset.csv')\n",
    "validation_data=pd.read_csv('../assets/processed dataset/validation dataset.csv')\n",
    "test_data=pd.read_csv('../assets/processed dataset/test dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61111111 0.19290466 0.20735786 0.21236002]\n",
      "[0.64285714 0.27937916 0.25139353 0.35296557]\n"
     ]
    }
   ],
   "source": [
    "#Data scaling\n",
    "scaler=MinMaxScaler()\n",
    "normalized_train_data=scaler.fit_transform(train_data)\n",
    "joblib.dump(scaler, \"../assets/scaler.gz\") \n",
    "print(normalized_train_data[0])\n",
    "normalized_validation_data=scaler.transform(validation_data)\n",
    "print(normalized_validation_data[0])\n",
    "normalized_test_data=scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10813, 180, 4) (10813, 4)\n",
      "(2176, 180, 4) (2176, 4)\n",
      "(2176, 180, 4) (2176, 4)\n",
      "torch.Size([10813, 180, 4]) torch.Size([10813, 4])\n",
      "torch.Size([2176, 180, 4]) torch.Size([2176, 4])\n",
      "torch.Size([2176, 180, 4]) torch.Size([2176, 4])\n"
     ]
    }
   ],
   "source": [
    "#Prepearing X and Y for training and validation\n",
    "# MAKING THE SEQUENCE: USING LOOK BACK PERIOD OF 60\n",
    "seq_length=180\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_validate=[]\n",
    "Y_validate=[]\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "for i in range(seq_length,train_data.shape[0]):\n",
    "    #Data from 0 to 59TH index\n",
    "    X_train.append(normalized_train_data[i-seq_length:i])\n",
    "    #T2M AT THE 60TH index\n",
    "    Y_train.append(normalized_train_data[i])\n",
    "\n",
    "for i in range(seq_length,validation_data.shape[0]):\n",
    "    #Data from 0 to 59TH index\n",
    "    X_validate.append(normalized_validation_data[i-seq_length:i])\n",
    "    #T2M AT THE 60TH index\n",
    "    Y_validate.append(normalized_validation_data[i])\n",
    "\n",
    "for i in range(seq_length,test_data.shape[0]):\n",
    "    #Data from 0 to 59TH index\n",
    "    X_test.append(normalized_test_data[i-seq_length:i])\n",
    "    #T2M AT THE 60TH index\n",
    "    Y_test.append(normalized_test_data[i])\n",
    "# converting into numpy arrays\n",
    "X_train,Y_train=np.array(X_train),np.array(Y_train)\n",
    "Y_train=Y_train.reshape(-1,4)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "\n",
    "X_test,Y_test=np.array(X_test),np.array(Y_test)\n",
    "Y_test=Y_test.reshape(-1,4)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "X_validate,Y_validate=np.array(X_validate),np.array(Y_validate)\n",
    "Y_validate=Y_validate.reshape(-1,4)\n",
    "print(X_validate.shape,Y_validate.shape)\n",
    "\n",
    "X_train = Variable(torch.Tensor(X_train))\n",
    "Y_train = Variable(torch.Tensor(Y_train))\n",
    "print(X_train.shape,Y_train.shape)\n",
    "\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "Y_test = Variable(torch.Tensor(Y_test))\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "X_validate = Variable(torch.Tensor(X_validate))\n",
    "Y_validate = Variable(torch.Tensor(Y_validate))\n",
    "print(X_validate.shape,Y_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single lstm Network definition\n",
    "class NETWORK_SINGLE_LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,num_classes,input_size,hidden_size,num_layers):\n",
    "        super(NETWORK_SINGLE_LSTM,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm=nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True).to(device)\n",
    "        self.fc=nn.Linear(hidden_size,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output_lstm, (h_n,c_n) = self.lstm(x)\n",
    "        input_fc=output_lstm[:,-1,:]\n",
    "        output_fc=self.fc(input_fc).to(device)\n",
    "        return output_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_curve(model, X_train, Y_train, num_epochs, learning_rate, model_name):\n",
    "# DEFINE OPTIMIZER\n",
    "    criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Lists to store training losses for plotting the training curve\n",
    "    train_losses = []\n",
    "    valid_losses=[]\n",
    "#batch gradient descent\n",
    "# TRAIN THE MODEL:\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        model.train()\n",
    "        outputs = model(X_train.to(device))\n",
    "        optimizer.zero_grad()\n",
    "    # obtain the loss function\n",
    "        loss = criterion(outputs, Y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_outputs=model(X_validate.to(device))\n",
    "            valid_loss=criterion(valid_outputs,Y_validate.to(device))\n",
    "            valid_losses.append(valid_loss.item())\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: %d, training loss: %1.5f, validation loss: %1.5f\" % (epoch, loss.item(),valid_loss.item()))\n",
    "# Plotting the training curve\n",
    "    epochs=range(1,num_epochs+1)\n",
    "    st=0;end=num_epochs\n",
    "    plt.plot( epochs[st:end],train_losses[st:end], label='Training Loss')\n",
    "    plt.plot(epochs[st:end],valid_losses[st:end], label='Validation Loss')\n",
    "    plt.title('Epoch vs Loss Curve - ' + model_name)\n",
    "\n",
    "# plt.plot(epochs, train_losses, label='Training Loss')\n",
    "# plt.plot(epochs, valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('../assets/losses/LSTM/'+model_name + 'LSTM_epoch_vs_loss.png')\n",
    "    plt.close()\n",
    "    torch.save(model.state_dict(),'../assets/trainedmodels/LSTM/'+model_name+'LSTM.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "num_epochs_list = [10]\n",
    "learning_rates = [0.01]\n",
    "hidden_sizes = [1]\n",
    "input_size=4\n",
    "num_classes=4\n",
    "num_layers_list = [1]\n",
    "# num_epochs_list = [50,100,200]\n",
    "# learning_rates = [0.01,0.001]\n",
    "# hidden_sizes = [4,8,16]\n",
    "# input_size=4\n",
    "# num_classes=4\n",
    "# num_layers_list = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_names = []\n",
    "hyperparameters = []\n",
    "mae_values = []\n",
    "\n",
    "for num_epochs in num_epochs_list:\n",
    "    for learning_rate in learning_rates:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for num_layers in num_layers_list:\n",
    "                model_name = f\"LSTM_epochs_{num_epochs}_lr_{learning_rate}_hidden_{hidden_size}_layers_{num_layers}\"\n",
    "                lstm_model = NETWORK_SINGLE_LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "                train_and_save_curve(lstm_model, X_train, Y_train, num_epochs, learning_rate, model_name)\n",
    "                \n",
    "                # Calculate predictions\n",
    "                predictions = lstm_model(X_test)\n",
    "                \n",
    "                # Inverse transform predictions to get denormalized values\n",
    "                predictions = predictions.detach().numpy()\n",
    "                \n",
    "                # Calculate MAE\n",
    "                mae = mean_absolute_error(Y_test, predictions)\n",
    "                \n",
    "                # Append model name, hyperparameters, and MAE to the lists\n",
    "                model_names.append(model_name)\n",
    "                hyperparameters.append((num_epochs, learning_rate, hidden_size, num_layers))\n",
    "                mae_values.append(mae)\n",
    "\n",
    "# Create a DataFrame with model names, hyperparameters, and MAE values\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Num_epochs': [params[0] for params in hyperparameters],\n",
    "    'Learning_rate': [params[1] for params in hyperparameters],\n",
    "    'Hidden_size': [params[2] for params in hyperparameters],\n",
    "    'Num_layers': [params[3] for params in hyperparameters],\n",
    "    'MAE': mae_values\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('./results/LSTM/mae_results_with_hyperparametersLSTM.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
